{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae9d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Initialize Ollama for LLM and embeddings (ensure Ollama server is running and models are pulled)\n",
    "# You can replace 'llama2' and 'nomic-embed-text' with other models you have.\n",
    "try:\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Error initializing Ollama. Make sure Ollama is running and models are pulled: {e}\")\n",
    "    st.stop() # Stop the app if Ollama cannot be initialized\n",
    "\n",
    "# Define some dummy documents for demonstration\n",
    "# In a real RAG app, these would come from a data source (e.g., loaded from files, databases)\n",
    "docs = [\n",
    "    \"The capital of France is Paris. Paris is known for its beautiful architecture.\",\n",
    "    \"Mount Everest is the highest mountain in the world, located in the Himalayas.\",\n",
    "    \"The Eiffel Tower is a famous landmark in Paris, France.\",\n",
    "    \"Deep learning is a subset of machine learning inspired by the structure of the human brain.\",\n",
    "    \"Python is a popular programming language for data science and AI.\"\n",
    "]\n",
    "\n",
    "# Create a FAISS vector store from the dummy documents\n",
    "# In a production environment, you would load an existing vector store\n",
    "try:\n",
    "    vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "except Exception as e:\n",
    "    st.error(f\"Error creating vector store: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "# --- LangChain Prompt and Chain Definition ---\n",
    "\n",
    "# Define the RAG prompt template\n",
    "# We use MessagesPlaceholder to dynamically inject chat history\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant. Answer the user's questions based on the provided context. If you don't know the answer, say 'I don't have enough information to answer that.'\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Placeholder for chat history\n",
    "        (\"user\", \"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the RAG chain\n",
    "# We define a function to format documents for the context\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: x[\"question\"]) | retriever | format_docs) # Retrieve context based on question\n",
    "    | prompt # Apply the prompt template\n",
    "    | llm # Pass to the LLM\n",
    "    | StrOutputParser() # Parse the output\n",
    ")\n",
    "\n",
    "# --- Streamlit Application ---\n",
    "\n",
    "st.set_page_config(page_title=\"RAG LLM Chat with History\", layout=\"centered\")\n",
    "st.title(\"RAG LLM Chat with History\")\n",
    "st.info(\"This RAG app uses Ollama for the LLM and embeddings, and stores chat history in Streamlit's session state. Ensure your Ollama server is running and models ('llama2', 'nomic-embed-text') are pulled.\")\n",
    "\n",
    "# Initialize chat history in Streamlit's session state if it doesn't exist\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Accept user input\n",
    "if prompt_input := st.chat_input(\"Ask a question about the documents...\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt_input})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt_input)\n",
    "\n",
    "    # Prepare chat history for the RAG chain\n",
    "    # We need to convert Streamlit's message format to LangChain's message format\n",
    "    langchain_chat_history = []\n",
    "    for msg in st.session_state.messages:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            langchain_chat_history.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            langchain_chat_history.append({\"role\": \"assistant\", \"content\": msg[\"content\"]})\n",
    "\n",
    "    # Call the RAG chain with the current question and chat history\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            # The rag_chain expects 'question' and 'chat_history' as input\n",
    "            response = rag_chain.invoke({\n",
    "                \"question\": prompt_input,\n",
    "                \"chat_history\": langchain_chat_history # Pass the entire chat history\n",
    "            })\n",
    "        st.markdown(response)\n",
    "\n",
    "    # Add assistant response to chat history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003579a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdbe39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65467d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030759c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-apl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
