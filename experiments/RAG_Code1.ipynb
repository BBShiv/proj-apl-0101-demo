{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fe8b16",
   "metadata": {},
   "source": [
    "SETTING UP THE ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636a3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eda485",
   "metadata": {},
   "source": [
    "IMPORTING DOC IN LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe7ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_9560\\3273384520.py:3: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  pdf_path =  \"G:\\Data Analyst CN\\Python\\LangChain\\Byte_Pair_Encoding.pdf\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='arXiv:1508.07909v5  [cs.CL]  10 Jun 2016\\nNeural Machine Translation of Rare Words with Subword Units\\nRico Sennrichand Barry Haddow and Alexandra Birch\\nSchool of Informatics, University of Edinburgh\\n{rico.sennrich,a.birch}@ed.ac.uk,bhaddow@inf.ed.ac.uk\\nAbstract\\nNeural machine translation (NMT) mod-\\nels typically operate with a ﬁxed vocabu-\\nlary, but translation is an open-vocabulary\\nproblem. Previous work addresses the\\ntranslation of out-of-vocabulary words by\\nbacking off to a dictionary. In this pa-\\nper, we introduce a simpler and more ef-\\nfective approach, making the NMT model\\ncapable of open-vocabulary translation by\\nencoding rare and unknown words as se-\\nquences of subword units. This is based on\\nthe intuition that various word classes are\\ntranslatable via smaller units than words,\\nfor instance names (via character copying\\nor transliteration), compounds (via com-\\npositional translation), and cognates and\\nloanwords (via phonological and morpho-\\nlogical transformations). We discuss the\\nsuitability of different word segmentation\\ntechniques, including simple charactern-\\ngram models and a segmentation based on\\nthebyte pair encodingcompression algo-\\nrithm, and empirically show that subword\\nmodels improve over a back-off dictionary\\nbaseline for the WMT 15 translation tasks\\nEnglish→German and English→Russian\\nby up to 1.1 and 1.3 BLEU , respectively.\\n1 Introduction\\nNeural machine translation has recently shown\\nimpressive results (Kalchbrenner and Blunsom,\\n2013; Sutskever et al., 2014; Bahdanau et al.,\\n2015). However, the translation of rare words\\nis an open problem. The vocabulary of neu-\\nral models is typically limited to 30 000–50 000\\nwords, but translation is an open-vocabulary prob-\\nThe research presented in this publication was conducted\\nin cooperation with Samsung Electronics Polska sp. z o.o. -\\nSamsung R&D Institute Poland.\\nlem, and especially for languages with produc-\\ntive word formation processes such as aggluti-\\nnation and compounding, translation models re-\\nquire mechanisms that go below the word level.\\nAs an example, consider compounds such as the\\nGerman Abwasser\\n|behandlungs|anlange ‘sewage\\nwater treatment plant’, for which a segmented,\\nvariable-length representation is intuitively more\\nappealing than encoding the word as a ﬁxed-length\\nvector.\\nFor word-level NMT models, the translation\\nof out-of-vocabulary words has been addressed\\nthrough a back-off to a dictionary look-up (Jean et\\nal., 2015; Luong et al., 2015b). We note that such\\ntechniques make assumptions that often do not\\nhold true in practice. For instance, there is not al-\\nways a 1-to-1 correspondence between source and\\ntarget words because of variance in the degree of\\nmorphological synthesis between languages, like\\nin our introductory compounding example. Also,\\nword-level models are unable to translate or gen-\\nerate unseen words. Copying unknown words into\\nthe target text, as done by (Jean et al., 2015; Luong\\net al., 2015b), is a reasonable strategy for names,\\nbut morphological changes and transliteration is\\noften required, especially if alphabets differ.\\nWe investigate NMT models that operate on the\\nlevel of subword units. Our main goal is to model\\nopen-vocabulary translation in the NMT network\\nitself, without requiring a back-off model for rare\\nwords. In addition to making the translation pro-\\ncess simpler, we also ﬁnd that the subword models\\nachieve better accuracy for the translation of rare\\nwords than large-vocabulary models and back-off\\ndictionaries, and are able to productively generate\\nnew words that were not seen at training time. Our\\nanalysis shows that the neural networks are able to\\nlearn compounding and transliteration from sub-\\nword representations.\\nThis paper has two main contributions:\\n• We show that open-vocabulary neural ma-'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='chine translation is possible by encoding\\n(rare) words via subword units. We ﬁnd our\\narchitecture simpler and more effective than\\nusing large vocabularies and back-off dictio-\\nnaries (Jean et al., 2015; Luong et al., 2015b).\\n• We adaptbyte pair encoding(BPE) (Gage,\\n1994), a compression algorithm, to the task\\nof word segmentation. BPE allows for the\\nrepresentation of an open vocabulary through\\na ﬁxed-size vocabulary of variable-length\\ncharacter sequences, making it a very suit-\\nable word segmentation strategy for neural\\nnetwork models.\\n2 Neural Machine Translation\\nWe follow the neural machine translation archi-\\ntecture by Bahdanau et al. (2015), which we will\\nbrieﬂy summarize here. However, we note that our\\napproach is not speciﬁc to this architecture.\\nThe neural machine translation system is imple-\\nmented as an encoder-decoder network with recur-\\nrent neural networks.\\nThe encoder is a bidirectional neural network\\nwith gated recurrent units (Cho et al., 2014)\\nthat reads an input sequencex = ( x1, ..., x m)\\nand calculates a forward sequence of hidden\\nstates(− →h 1, ..., − →h m), and a backward sequence\\n(← −h 1, ..., ← −h m). The hidden states− →h j and ← −h j are\\nconcatenated to obtain the annotation vectorhj .\\nThe decoder is a recurrent neural network that\\npredicts a target sequencey = (y1, ..., y n). Each\\nword yi is predicted based on a recurrent hidden\\nstatesi, the previously predicted wordyi−1, and\\na context vectorci. ci is computed as a weighted\\nsum of the annotationshj . The weight of each\\nannotationhj is computed through analignment\\nmodel α ij , which models the probability thatyi is\\naligned toxj . The alignment model is a single-\\nlayer feedforward neural network that is learned\\njointly with the rest of the network through back-\\npropagation.\\nA detailed description can be found in (Bah-\\ndanau et al., 2015). Training is performed on a\\nparallel corpus with stochastic gradient descent.\\nFor translation, a beam search with small beam\\nsize is employed.\\n3 Subword Translation\\nThe main motivation behind this paper is that\\nthe translation of some words is transparent in\\nthat they are translatable by a competent transla-\\ntor even if they are novel to him or her, based\\non a translation of known subword units such as\\nmorphemes or phonemes. Word categories whose\\ntranslation is potentially transparent include:\\n• named entities. Between languages that share\\nan alphabet, names can often be copied from\\nsource to target text. Transcription or translit-\\neration may be required, especially if the al-\\nphabets or syllabaries differ. Example:\\nBarack Obama (English; German)\\nБарак Обама (Russian)\\nバラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)\\n• cognates and loanwords. Cognates and loan-\\nwords with a common origin can differ in\\nregular ways between languages, so that\\ncharacter-level translation rules are sufﬁcient\\n(Tiedemann, 2012). Example:\\nclaustrophobia (English)\\nKlaustrophobie (German)\\nКлаустрофобия (Klaustrofobiâ) (Russian)\\n• morphologically complex words. Words con-\\ntaining multiple morphemes, for instance\\nformed via compounding, afﬁxation, or in-\\nﬂection, may be translatable by translating\\nthe morphemes separately. Example:\\nsolar system (English)\\nSonnensystem (Sonne + System) (German)\\nNaprendszer (Nap + Rendszer) (Hungarian)\\nIn an analysis of 100 rare tokens (not among\\nthe 50 000 most frequent types) in our German\\ntraining data1, the majority of tokens are poten-\\ntially translatable from English through smaller\\nunits. We ﬁnd 56 compounds, 21 names,\\n6 loanwords with a common origin (emanci-\\npate→emanzipieren), 5 cases of transparent afﬁx-\\nation (sweetish‘sweet’ + ‘-ish’→süßlich‘süß’ +\\n‘-lich’), 1 number and 1 computer language iden-\\ntiﬁer.\\nOur hypothesis is that a segmentation of rare\\nwords into appropriate subword units is sufﬁ-\\ncient to allow for the neural translation network\\nto learn transparent translations, and to general-\\nize this knowledge to translate and produce unseen\\nwords.2 We provide empirical support for this hy-\\n1Primarily parliamentary proceedings and web crawl data.\\n2Not every segmentation we produce is transparent.\\nWhile we expect no performance beneﬁt from opaque seg-\\nmentations, i.e. segmentations where the units cannot be\\ntranslated independently, our NMT models show robustness\\ntowards oversplitting.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='pothesis in Sections 4 and 5. First, we discuss dif-\\nferent subword representations.\\n3.1 Related Work\\nFor Statistical Machine Translation (SMT), the\\ntranslation of unknown words has been the subject\\nof intensive research.\\nA large proportion of unknown words are\\nnames, which can just be copied into the tar-\\nget text if both languages share an alphabet. If\\nalphabets differ, transliteration is required (Dur-\\nrani et al., 2014). Character-based translation has\\nalso been investigated with phrase-based models,\\nwhich proved especially successful for closely re-\\nlated languages (Vilar et al., 2007; Tiedemann,\\n2009; Neubig et al., 2012).\\nThe segmentation of morphologically complex\\nwords such as compounds is widely used for SMT,\\nand various algorithms for morpheme segmen-\\ntation have been investigated (Nießen and Ney,\\n2000; Koehn and Knight, 2003; Virpioja et al.,\\n2007; Stallard et al., 2012). Segmentation al-\\ngorithms commonly used for phrase-based SMT\\ntend to be conservative in their splitting decisions,\\nwhereas we aim for an aggressive segmentation\\nthat allows for open-vocabulary translation with a\\ncompact network vocabulary, and without having\\nto resort to back-off dictionaries.\\nThe best choice of subword units may be task-\\nspeciﬁc. For speech recognition, phone-level lan-\\nguage models have been used (Bazzi and Glass,\\n2000). Mikolov et al. (2012) investigate subword\\nlanguage models, and propose to use syllables.\\nFor multilingual segmentation tasks, multilingual\\nalgorithms have been proposed (Snyder and Barzi-\\nlay, 2008). We ﬁnd these intriguing, but inapplica-\\nble at test time.\\nVarious techniques have been proposed to pro-\\nduce ﬁxed-length continuous word vectors based\\non characters or morphemes (Luong et al., 2013;\\nBotha and Blunsom, 2014; Ling et al., 2015a; Kim\\net al., 2015). An effort to apply such techniques\\nto NMT, parallel to ours, has found no signiﬁcant\\nimprovement over word-based approaches (Ling\\net al., 2015b). One technical difference from our\\nwork is that the attention mechanism still oper-\\nates on the level of words in the model by Ling\\net al. (2015b), and that the representation of each\\nword is ﬁxed-length. We expect that the attention\\nmechanism beneﬁts from our variable-length rep-\\nresentation: the network can learn to place atten-\\ntion on different subword units at each step. Re-\\ncall our introductory exampleAbwasserbehand-\\nlungsanlange, for which a subword segmentation\\navoids the information bottleneck of a ﬁxed-length\\nrepresentation.\\nNeural machine translation differs from phrase-\\nbased methods in that there are strong incentives to\\nminimize the vocabulary size of neural models to\\nincrease time and space efﬁciency, and to allow for\\ntranslation without back-off models. At the same\\ntime, we also want a compact representation of the\\ntext itself, since an increase in text length reduces\\nefﬁciency and increases the distances over which\\nneural models need to pass information.\\nA simple method to manipulate the trade-off be-\\ntween vocabulary size and text size is to use short-\\nlists of unsegmented words, using subword units\\nonly for rare words. As an alternative, we pro-\\npose a segmentation algorithm based on byte pair\\nencoding (BPE), which lets us learn a vocabulary\\nthat provides a good compression rate of the text.\\n3.2 Byte Pair Encoding (BPE)\\nByte Pair Encoding (BPE) (Gage, 1994) is a sim-\\nple data compression technique that iteratively re-\\nplaces the most frequent pair of bytes in a se-\\nquence with a single, unused byte. We adapt this\\nalgorithm for word segmentation. Instead of merg-\\ning frequent pairs of bytes, we merge characters or\\ncharacter sequences.\\nFirstly, we initialize the symbol vocabulary with\\nthe character vocabulary, and represent each word\\nas a sequence of characters, plus a special end-of-\\nword symbol ‘·’, which allows us to restore the\\noriginal tokenization after translation. We itera-\\ntively count all symbol pairs and replace each oc-\\ncurrence of the most frequent pair (‘A’, ‘B’) with\\na new symbol ‘AB’. Each merge operation pro-\\nduces a new symbol which represents a charac-\\ntern-gram. Frequent charactern-grams (or whole\\nwords) are eventually merged into a single sym-\\nbol, thus BPE requires no shortlist. The ﬁnal sym-\\nbol vocabulary size is equal to the size of the initial\\nvocabulary, plus the number of merge operations\\n– the latter is the only hyperparameter of the algo-\\nrithm.\\nFor efﬁciency, we do not consider pairs that\\ncross word boundaries. The algorithm can thus be\\nrun on the dictionary extracted from a text, with\\neach word being weighted by its frequency. A\\nminimal Python implementation is shown in Al-'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content=\"Algorithm 1Learn BPE operations\\nimportre, collections\\ndefget_stats(vocab):\\npairs = collections.defaultdict(int)\\nforword, freqinvocab.items():\\nsymbols = word.split()\\nforiinrange(len(symbols)-1):\\npairs[symbols[i],symbols[i+1]] += freq\\nreturnpairs\\ndefmerge_vocab(pair, v_in):\\nv_out = {}\\nbigram = re.escape(' '.join(pair))\\np = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)')\\nforwordinv_in:\\nw_out = p.sub(''.join(pair), word)\\nv_out[w_out] = v_in[word]\\nreturnv_out\\nvocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\\n'n e w e s t </w>':6, 'w i d e s t </w>':3}\\nnum_merges = 10\\nforiinrange(num_merges):\\npairs = get_stats(vocab)\\nbest = max(pairs, key=pairs.get)\\nvocab = merge_vocab(best, vocab)\\nprint(best)\\nr· → r·\\nl o → lo\\nlo w → low\\ne r· → er·\\nFigure 1: BPE merge operations learned from dic-\\ntionary {‘low’, ‘lowest’, ‘newer’, ‘wider’}.\\ngorithm 1. In practice, we increase efﬁciency by\\nindexing all pairs, and updating data structures in-\\ncrementally.\\nThe main difference to other compression al-\\ngorithms, such as Huffman encoding, which have\\nbeen proposed to produce a variable-length en-\\ncoding of words for NMT (Chitnis and DeNero,\\n2015), is that our symbol sequences are still in-\\nterpretable as subword units, and that the network\\ncan generalize to translate and produce new words\\n(unseen at training time) on the basis of these sub-\\nword units.\\nFigure 1 shows a toy example of learned BPE\\noperations. At test time, we ﬁrst split words into\\nsequences of characters, then apply the learned op-\\nerations to merge the characters into larger, known\\nsymbols. This is applicable to any word, and\\nallows for open-vocabulary networks with ﬁxed\\nsymbol vocabularies.3 In our example, the OOV\\n‘lower’ would be segmented into ‘low er·’.\\n3The only symbols that will be unknown at test time are\\nunknown characters, or symbols of which all occurrences\\nin the training text have been merged into larger symbols,\\nlike ‘safeguar’, which has all occurrences in our training text\\nmerged into ‘safeguard’. We observed no such symbols at\\ntest time, but the issue could be easily solved by recursively\\nreversing speciﬁc merges until all symbols are known.\\nWe evaluate two methods of applying BPE:\\nlearning two independent encodings, one for the\\nsource, one for the target vocabulary, or learning\\nthe encoding on the union of the two vocabular-\\nies (which we calljoint BPE).4 The former has the\\nadvantage of being more compact in terms of text\\nand vocabulary size, and having stronger guaran-\\ntees that each subword unit has been seen in the\\ntraining text of the respective language, whereas\\nthe latter improves consistency between the source\\nand the target segmentation. If we apply BPE in-\\ndependently, the same name may be segmented\\ndifferently in the two languages, which makes it\\nharder for the neural models to learn a mapping\\nbetween the subword units. To increase the con-\\nsistency between English and Russian segmenta-\\ntion despite the differing alphabets, we transliter-\\nate the Russian vocabulary into Latin characters\\nwith ISO-9 to learn the joint BPE encoding, then\\ntransliterate the BPE merge operations back into\\nCyrillic to apply them to the Russian training text.5\\n4 Evaluation\\nWe aim to answer the following empirical ques-\\ntions:\\n• Can we improve the translation of rare and\\nunseen words in neural machine translation\\nby representing them via subword units?\\n• Which segmentation into subword units per-\\nforms best in terms of vocabulary size, text\\nsize, and translation quality?\\nWe perform experiments on data from the\\nshared translation task of WMT 2015. For\\nEnglish→German, our training set consists of 4.2\\nmillion sentence pairs, or approximately 100 mil-\\nlion tokens. For English→Russian, the training set\\nconsists of 2.6 million sentence pairs, or approx-\\nimately 50 million tokens. We tokenize and true-\\ncase the data with the scripts provided in Moses\\n(Koehn et al., 2007). We use newstest2013 as de-\\nvelopment set, and report results on newstest2014\\nand newstest2015.\\nWe report results with BLEU (mteval-v13a.pl),\\nand CHR F3 (Popovi´c, 2015), a character n-gram\\nF3 score which was found to correlate well with\\n4In practice, we simply concatenate the source and target\\nside of the training set to learn joint BPE.\\n5Since the Russian training text also contains words that\\nuse the Latin alphabet, we also apply the Latin BPE opera-\\ntions.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='human judgments, especially for translations out\\nof English (Stanojevi´c et al., 2015). Since our\\nmain claim is concerned with the translation of\\nrare and unseen words, we report separate statis-\\ntics for these. We measure these through unigram\\nF1, which we calculate as the harmonic mean of\\nclipped unigram precision and recall.6\\nWe perform all experiments with Groundhog7\\n(Bahdanau et al., 2015). We generally follow set-\\ntings by previous work (Bahdanau et al., 2015;\\nJean et al., 2015). All networks have a hidden\\nlayer size of 1000, and an embedding layer size\\nof 620. Following Jean et al. (2015), we only keep\\na shortlist ofτ = 30000words in memory.\\nDuring training, we use Adadelta (Zeiler, 2012),\\na minibatch size of 80, and reshufﬂe the train-\\ning set between epochs. We train a network for\\napproximately 7 days, then take the last 4 saved\\nmodels (models being saved every 12 hours), and\\ncontinue training each with a ﬁxed embedding\\nlayer (as suggested by (Jean et al., 2015)) for 12\\nhours. We perform two independent training runs\\nfor each models, once with cut-off for gradient\\nclipping (Pascanu et al., 2013) of 5.0, once with\\na cut-off of 1.0 – the latter produced better single\\nmodels for most settings. We report results of the\\nsystem that performed best on our development set\\n(newstest2013), and of an ensemble of all 8 mod-\\nels.\\nWe use a beam size of 12 for beam search,\\nwith probabilities normalized by sentence length.\\nWe use a bilingual dictionary based on fast-align\\n(Dyer et al., 2013). For our baseline, this serves\\nas back-off dictionary for rare words. We also use\\nthe dictionary to speed up translation for all ex-\\nperiments, only performing the softmax over a ﬁl-\\ntered list of candidate translations (like Jean et al.\\n(2015), we useK = 30000;K′ = 10).\\n4.1 Subword statistics\\nApart from translation quality, which we will ver-\\nify empirically, our main objective is to represent\\nan open vocabulary through a compact ﬁxed-size\\nsubword vocabulary, and allow for efﬁcient train-\\ning and decoding.8\\nStatistics for different segmentations of the Ger-\\n6Clipped unigram precision is essentially 1-gram BLEU\\nwithout brevity penalty.\\n7github.com/sebastien-j/LV_groundhog\\n8The time complexity of encoder-decoder architectures is\\nat least linear to sequence length, and oversplitting harmsef-\\nﬁciency.\\nman side of the parallel data are shown in Table\\n1. A simple baseline is the segmentation of words\\ninto charactern-grams.9 Charactern-grams allow\\nfor different trade-offs between sequence length\\n(# tokens) and vocabulary size (# types), depend-\\ning on the choice ofn. The increase in sequence\\nlength is substantial; one way to reduce sequence\\nlength is to leave a shortlist of thek most frequent\\nword types unsegmented. Only the unigram repre-\\nsentation is truly open-vocabulary. However, the\\nunigram representation performed poorly in pre-\\nliminary experiments, and we report translation re-\\nsults with a bigram representation, which is empir-\\nically better, but unable to produce some tokens in\\nthe test set with the training set vocabulary.\\nWe report statistics for several word segmenta-\\ntion techniques that have proven useful in previous\\nSMT research, including frequency-based com-\\npound splitting (Koehn and Knight, 2003), rule-\\nbased hyphenation (Liang, 1983), and Morfessor\\n(Creutz and Lagus, 2002). We ﬁnd that they only\\nmoderately reduce vocabulary size, and do not\\nsolve the unknown word problem, and we thus ﬁnd\\nthem unsuitable for our goal of open-vocabulary\\ntranslation without back-off dictionary.\\nBPE meets our goal of being open-vocabulary,\\nand the learned merge operations can be applied\\nto the test set to obtain a segmentation with no\\nunknown symbols.10 Its main difference from\\nthe character-level model is that the more com-\\npact representation of BPE allows for shorter se-\\nquences, and that the attention model operates\\non variable-length units.11 Table 1 shows BPE\\nwith 59 500 merge operations, and joint BPE with\\n89 500 operations.\\nIn practice, we did not include infrequent sub-\\nword units in the NMT network vocabulary, since\\nthere is noise in the subword symbol sets, e.g.\\nbecause of characters from foreign alphabets.\\nHence, our network vocabularies in Table 2 are\\ntypically slightly smaller than the number of types\\nin Table 1.\\n9Our character n-grams do not cross word boundaries. We\\nmark whether a subword is word-ﬁnal or not with a special\\ncharacter, which allows us to restore the original tokenization.\\n10Joint BPE can produce segments that are unknown be-\\ncause they only occur in the English training text, but these\\nare rare (0.05% of test tokens).\\n11We highlighted the limitations of word-level attention in\\nsection 3.1. At the other end of the spectrum, the character\\nlevel is suboptimal for alignment (Tiedemann, 2009).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='vocabulary B LEU CHR F3 unigram F1 (%)\\nname segmentation shortlist source target single ens-8single ens-8 all rare OOV\\nsyntax-based (Sennrich and Haddow, 2015) 24.4 - 55.3 - 59.1 46.0 37.7\\nWUnk - - 300 000 500 000 20.6 22.8 47.2 48.9 56.7 20.4 0.0\\nWDict - - 300 000 500 000 22.0 24.2 50.5 52.4 58.1 36.8 36.8\\nC2-50k char-bigram 50 000 60 000 60 000 22.8 25.3 51.9 53.5 58.4 40.5 30.9\\nBPE-60k BPE - 60 000 60 000 21.5 24.5 52.0 53.9 58.4 40.9 29.3\\nBPE-J90k BPE (joint) - 90 000 90 000 22.8 24.7 51.7 54.1 58.5 41.8 33.6\\nTable 2: English→German translation performance (BLEU ,CHR F3 and unigram F1) on newstest2015.\\nEns-8: ensemble of 8 models. Best NMT system in bold. UnigramF1 (with ensembles) is computed for\\nall words (n = 44085), rare words (not among top 50 000 in training set;n = 2900), and OOVs (not in\\ntraining set;n = 1168).\\nsegmentation # tokens # types # UNK\\nnone 100 m 1 750 000 1079\\ncharacters 550 m 3000 0\\ncharacter bigrams 306 m 20 000 34\\ncharacter trigrams 214 m 120 000 59\\ncompound splitting△ 102 m 1 100 000 643\\nmorfessor* 109 m 544 000 237\\nhyphenation⋄ 186 m 404 000 230\\nBPE 112 m 63 000 0\\nBPE (joint) 111 m 82 000 32\\ncharacter bigrams 129 m 69 000 34(shortlist: 50 000)\\nTable 1: Corpus statistics for German training\\ncorpus with different word segmentation tech-\\nniques. #UNK: number of unknown tokens in\\nnewstest2013.△: (Koehn and Knight, 2003); *:\\n(Creutz and Lagus, 2002);⋄: (Liang, 1983).\\n4.2 Translation experiments\\nEnglish→German translation results are shown in\\nTable 2; English→Russian results in Table 3.\\nOur baselineWDict is a word-level model with\\na back-off dictionary. It differs fromWUnk in that\\nthe latter uses no back-off dictionary, and just rep-\\nresents out-of-vocabulary words as UNK12. The\\nback-off dictionary improves unigram F1 for rare\\nand unseen words, although the improvement is\\nsmaller for English→Russian, since the back-off\\ndictionary is incapable of transliterating names.\\nAll subword systems operate without a back-off\\ndictionary. We ﬁrst focus on unigram F1, where\\nall systems improve over the baseline, especially\\nfor rare words (36.8%→41.8% for EN →DE;\\n26.5%→29.7% for EN →RU). For OOVs, the\\nbaseline strategy of copying unknown words\\nworks well for English→German. However, when\\nalphabets differ, like in English→Russian, the\\nsubword models do much better.\\n12We use UNK for words that are outside the model vo-\\ncabulary, andOOV for those that do not occur in the training\\ntext.\\nUnigram F1 scores indicate that learning the\\nBPE symbols on the vocabulary union (BPE-\\nJ90k) is more effective than learning them sep-\\narately (BPE-60k ), and more effective than using\\ncharacter bigrams with a shortlist of 50 000 unseg-\\nmented words (C2-50k), but all reported subword\\nsegmentations are viable choices and outperform\\nthe back-off dictionary baseline.\\nOur subword representations cause big im-\\nprovements in the translation of rare and unseen\\nwords, but these only constitute 9-11% of the test\\nsets. Since rare words tend to carry central in-\\nformation in a sentence, we suspect that BLEU\\nand CHR F3 underestimate their effect on transla-\\ntion quality. Still, we also see improvements over\\nthe baseline in total unigram F1, as well as BLEU\\nand CHR F3, and the subword ensembles outper-\\nform the WDict baseline by 0.3–1.3 BLEU and\\n0.6–2 CHR F3. There is some inconsistency be-\\ntween BLEU and CHR F3, which we attribute to the\\nfact that BLEU has a precision bias, andCHR F3 a\\nrecall bias.\\nFor English→German, we observe the best\\nB LEU score of 25.3 with C2-50k, but the best\\nCHR F3 score of 54.1 with BPE-J90k. For com-\\nparison to the (to our knowledge) best non-neural\\nMT system on this data set, we report syntax-\\nbased SMT results (Sennrich and Haddow, 2015).\\nWe observe that our best systems outperform the\\nsyntax-based system in terms of BLEU , but not\\nin terms ofCHR F3. Regarding other neural sys-\\ntems, Luong et al. (2015a) report a BLEU score of\\n25.9 on newstest2015, but we note that they use an\\nensemble of 8 independently trained models, and\\nalso report strong improvements from applying\\ndropout, which we did not use. We are conﬁdent\\nthat our improvements to the translation of rare\\nwords are orthogonal to improvements achievable\\nthrough other improvements in the network archi-'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='tecture, training algorithm, or better ensembles.\\nFor English→Russian, the state of the art is\\nthe phrase-based system by Haddow et al. (2015).\\nIt outperforms our WDict baseline by 1.5 BLEU .\\nThe subword models are a step towards closing\\nthis gap, and BPE-J90k yields an improvement of\\n1.3 BLEU , and 2.0CHR F3, over WDict.\\nAs a further comment on our translation results,\\nwe want to emphasize that performance variabil-\\nity is still an open problem with NMT. On our de-\\nvelopment set, we observe differences of up to 1\\nB LEU between different models. For single sys-\\ntems, we report the results of the model that per-\\nforms best on dev (out of 8), which has a stabi-\\nlizing effect, but how to control for randomness\\ndeserves further attention in future research.\\n5 Analysis\\n5.1 Unigram accuracy\\nOur main claims are that the translation of rare and\\nunknown words is poor in word-level NMT mod-\\nels, and that subword models improve the trans-\\nlation of these word types. To further illustrate\\nthe effect of different subword segmentations on\\nthe translation of rare and unseen words, we plot\\ntarget-side words sorted by their frequency in the\\ntraining set.13 To analyze the effect of vocabulary\\nsize, we also include the systemC2-3/500k, which\\nis a system with the same vocabulary size as the\\nWDict baseline, and character bigrams to repre-\\nsent unseen words.\\nFigure 2 shows results for the English–German\\nensemble systems on newstest2015. Unigram\\nF1 of all systems tends to decrease for lower-\\nfrequency words. The baseline system has a spike\\nin F1 for OOVs, i.e. words that do not occur in\\nthe training text. This is because a high propor-\\ntion of OOVs are names, for which a copy from\\nthe source to the target text is a good strategy for\\nEnglish→German.\\nThe systems with a target vocabulary of 500 000\\nwords mostly differ in how well they translate\\nwords with rank > 500 000. A back-off dictionary\\nis an obvious improvement over producing UNK,\\nbut the subword system C2-3/500k achieves better\\nperformance. Note that all OOVs that the back-\\noff dictionary produces are words that are copied\\nfrom the source, usually names, while the subword\\n13We perform binning of words with the same training set\\nfrequency, and apply bezier smoothing to the graph.\\nsystems can productively form new words such as\\ncompounds.\\nFor the 50 000 most frequent words, the repre-\\nsentation is the same for all neural networks, and\\nall neural networks achieve comparable unigram\\nF1 for this category. For the interval between fre-\\nquency rank 50 000 and 500 000, the comparison\\nbetween C2-3/500k and C2-50k unveils an inter-\\nesting difference. The two systems only differ in\\nthe size of the shortlist, with C2-3/500k represent-\\ning words in this interval as single units, and C2-\\n50k via subword units. We ﬁnd that the perfor-\\nmance of C2-3/500k degrades heavily up to fre-\\nquency rank 500 000, at which point the model\\nswitches to a subword representation and perfor-\\nmance recovers. The performance of C2-50k re-\\nmains more stable. We attribute this to the fact\\nthat subword units are less sparse than words. In\\nour training set, the frequency rank 50 000 corre-\\nsponds to a frequency of 60 in the training data;\\nthe frequency rank 500 000 to a frequency of 2.\\nBecause subword representations are less sparse,\\nreducing the size of the network vocabulary, and\\nrepresenting more words via subword units, can\\nlead to better performance.\\nThe F1 numbers hide some qualitative differ-\\nences between systems. For English→German,\\nWDict produces few OOVs (26.5% recall), but\\nwith high precision (60.6%) , whereas the subword\\nsystems achieve higher recall, but lower precision.\\nWe note that the character bigram model C2-50k\\nproduces the most OOV words, and achieves rel-\\natively low precision of 29.1% for this category.\\nHowever, it outperforms the back-off dictionary\\nin recall (33.0%). BPE-60k, which suffers from\\ntransliteration (or copy) errors due to segmenta-\\ntion inconsistencies, obtains a slightly better pre-\\ncision (32.4%), but a worse recall (26.6%). In con-\\ntrast to BPE-60k, the joint BPE encoding of BPE-\\nJ90k improves both precision (38.6%) and recall\\n(29.8%).\\nFor English→Russian, unknown names can\\nonly rarely be copied, and usually require translit-\\neration. Consequently, the WDict baseline per-\\nforms more poorly for OOVs (9.2% precision;\\n5.2% recall), and the subword models improve\\nboth precision and recall (21.9% precision and\\n15.6% recall for BPE-J90k). The full unigram F1\\nplot is shown in Figure 3.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='vocabulary B LEU CHR F3 unigram F1 (%)\\nname segmentation shortlist source target single ens-8single ens-8 all rare OOV\\nphrase-based (Haddow et al., 2015) 24.3 - 53.8 - 56.0 31.3 16.5\\nWUnk - - 300 000 500 000 18.8 22.4 46.5 49.9 54.2 25.2 0.0\\nWDict - - 300 000 500 000 19.1 22.8 47.5 51.0 54.8 26.5 6.6\\nC2-50k char-bigram 50 000 60 000 60 000 20.9 24.1 49.0 51.6 55.2 27.8 17.4\\nBPE-60k BPE - 60 000 60 000 20.5 23.6 49.8 52.7 55.3 29.7 15.6\\nBPE-J90k BPE (joint) - 90 000 100 000 20.4 24.1 49.7 53.0 55.8 29.7 18.3\\nTable 3: English→Russian translation performance (BLEU ,CHR F3 and unigram F1) on newstest2015.\\nEns-8: ensemble of 8 models. Best NMT system in bold. UnigramF1 (with ensembles) is computed for\\nall words (n = 55654), rare words (not among top 50 000 in training set;n = 5442), and OOVs (not in\\ntraining set;n = 851).\\n100 101 102 103 104 105 1060\\n0. 2\\n0. 4\\n0. 6\\n0. 8\\n1\\n50 000 500 000\\ntraining set frequency rank\\nunigram F1\\nBPE-J90k\\nC2-50k\\nC2-300/500k\\nWDict\\nWUnk\\nFigure 2: English→German unigram F1 on new-\\nstest2015 plotted by training set frequency rank\\nfor different NMT systems.\\n100 101 102 103 104 105 1060\\n0. 2\\n0. 4\\n0. 6\\n0. 8\\n1\\n50 000 500 000\\ntraining set frequency rank\\nunigram F1\\nBPE-J90k\\nC2-50k\\nWDict\\nWUnk\\nFigure 3: English→Russian unigram F1 on new-\\nstest2015 plotted by training set frequency rank\\nfor different NMT systems.\\n5.2 Manual Analysis\\nTable 4 shows two translation examples for\\nthe translation direction English→German, Ta-\\nble 5 for English→Russian. The baseline sys-\\ntem fails for all of the examples, either by delet-\\ning content (health), or by copying source words\\nthat should be translated or transliterated. The\\nsubword translations ofhealth research insti-\\ntutesshow that the subword systems are capa-\\nble of learning translations when oversplitting (re-\\nsearch→Fo\\n|rs|ch|un|g), or when the segmentation\\ndoes not match morpheme boundaries: the seg-\\nmentationForschungs\\n|institutenwould be linguis-\\ntically more plausible, and simpler to align to the\\nEnglishresearch institutes, than the segmentation\\nForsch\\n|ungsinstitu|tenin the BPE-60k system, but\\nstill, a correct translation is produced. If the sys-\\ntems have failed to learn a translation due to data\\nsparseness, like forasinine, which should be trans-\\nlated asdumm , we see translations that are wrong,\\nbut could be plausible for (partial) loanwords (asi-\\nnine Situation→Asinin-Situation).\\nThe English→Russian examples show that\\nthe subword systems are capable of translitera-\\ntion. However, transliteration errors do occur,\\neither due to ambiguous transliterations, or be-\\ncause of non-consistent segmentations between\\nsource and target text which make it hard for\\nthe system to learn a transliteration mapping.\\nNote that the BPE-60k system encodesMirza-\\nyeva inconsistently for the two language pairs\\n(Mirz\\n|ayeva→Мир|за|ева Mir|za|eva). This ex-\\nample is still translated correctly, but we observe\\nspurious insertions and deletions of characters in\\nthe BPE-60k system. An example is the translit-\\neration ofrakﬁsk, where aп is inserted and aк\\nis deleted. We trace this error back to transla-\\ntion pairs in the training data with inconsistent\\nsegmentations, such as (p\\n|rak|ri|ti→пра|крит|и'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='system sentence\\nsource health research institutes\\nreference Gesundheitsforschungsinstitute\\nWDict Forschungsinstitute\\nC2-50k Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n\\nBPE-60k Gesundheits|forsch|ungsinstitu|ten\\nBPE-J90k Gesundheits|forsch|ungsin|stitute\\nsource asinine situation\\nreference dumme Situation\\nWDict asinine situation→ UNK → asinine\\nC2-50k as|in|in|e situation→ As|in|en|si|tu|at|io|n\\nBPE-60k as|in|ine situation→ A |in|line-|Situation\\nBPE-J90K as|in|ine situation→ As|in|in-|Situation\\nTable 4: English→German translation example.\\n“|” marks subword boundaries.\\nsystem sentence\\nsource Mirzayeva\\nreference Мирзаева (Mirzaeva)\\nWDict Mirzayeva → UNK → Mirzayeva\\nC2-50k Mi |rz|ay|ev|a → Ми|рз|ае|ва (Mi|rz|ae|va)\\nBPE-60k Mirz|ayeva → Мир|за|ева (Mir|za|eva)\\nBPE-J90k Mir|za|yeva → Мир|за|ева (Mir|za|eva)\\nsource rakﬁsk\\nreference ракфиска (rakﬁska)\\nWDict rakﬁsk → UNK → rakﬁsk\\nC2-50k ra|kf|is|k → ра|кф|ис|к (ra|kf|is|k)\\nBPE-60k rak|f|isk→ пра|ф|иск (pra|f|isk)\\nBPE-J90k rak|f|isk→ рак|ф|иска (rak|f|iska)\\nTable 5: English→Russian translation examples.\\n“|” marks subword boundaries.\\n(pra|krit|i)), from which the translation (rak→пра)\\nis erroneously learned. The segmentation of the\\njoint BPE system (BPE-J90k) is more consistent\\n(pra\\n|krit|i→пра|крит|и (pra|krit|i)).\\n6 Conclusion\\nThe main contribution of this paper is that we\\nshow that neural machine translation systems are\\ncapable of open-vocabulary translation by repre-\\nsenting rare and unseen words as a sequence of\\nsubword units.14 This is both simpler and more\\neffective than using a back-off translation model.\\nWe introduce a variant of byte pair encoding for\\nword segmentation, which is capable of encod-\\ning open vocabularies with a compact symbol vo-\\ncabulary of variable-length subword units. We\\nshow performance gains over the baseline with\\nboth BPE segmentation, and a simple character bi-\\ngram segmentation.\\nOur analysis shows that not only out-of-\\nvocabulary words, but also rare in-vocabulary\\nwords are translated poorly by our baseline NMT\\n14The source code of the segmentation algorithms\\nis available athttps://github.com/rsennrich/\\nsubword-nmt.\\nsystem, and that reducing the vocabulary size\\nof subword models can actually improve perfor-\\nmance. In this work, our choice of vocabulary size\\nis somewhat arbitrary, and mainly motivated by\\ncomparison to prior work. One avenue of future\\nresearch is to learn the optimal vocabulary size for\\na translation task, which we expect to depend on\\nthe language pair and amount of training data, au-\\ntomatically. We also believe there is further po-\\ntential in bilingually informed segmentation algo-\\nrithms to create more alignable subword units, al-\\nthough the segmentation algorithm cannot rely on\\nthe target text at runtime.\\nWhile the relative effectiveness will depend on\\nlanguage-speciﬁc factors such as vocabulary size,\\nwe believe that subword segmentations are suit-\\nable for most language pairs, eliminating the need\\nfor large NMT vocabularies or back-off models.\\nAcknowledgments\\nWe thank Maja Popovi ´c for her implementa-\\ntion ofCHR F, with which we veriﬁed our re-\\nimplementation. The research presented in this\\npublication was conducted in cooperation with\\nSamsung Electronics Polska sp. z o.o. - Sam-\\nsung R&D Institute Poland. This project received\\nfunding from the European Union’s Horizon 2020\\nresearch and innovation programme under grant\\nagreement 645452 (QT21).\\nReferences\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2015. Neural Machine Translation by Jointly\\nLearning to Align and Translate. InProceedings of\\nthe International Conference on Learning Represen-\\ntations (ICLR).\\nIssam Bazzi and James R. Glass. 2000. Modeling out-\\nof-vocabulary words for robust speech recognition.\\nIn Sixth International Conference on Spoken Lan-\\nguage Processing, ICSLP 2000 / INTERSPEECH\\n2000, pages 401–404, Beijing, China.\\nJan A. Botha and Phil Blunsom. 2014. Compositional\\nMorphology for Word Representations and Lan-\\nguage Modelling. InProceedings of the 31st Inter-\\nnational Conference on Machine Learning (ICML),\\nBeijing, China.\\nRohan Chitnis and John DeNero. 2015. Variable-\\nLength Word Encodings for Neural Translation\\nModels. InProceedings of the 2015 Conference on\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-\\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\\nger Schwenk, and Yoshua Bengio. 2014. Learn-\\ning Phrase Representations using RNN Encoder–\\nDecoder for Statistical Machine Translation. InPro-\\nceedings of the 2014 Conference on Empirical Meth-\\nods in Natural Language Processing (EMNLP),\\npages 1724–1734, Doha, Qatar. Association for\\nComputational Linguistics.\\nMathias Creutz and Krista Lagus. 2002. Unsupervised\\nDiscovery of Morphemes. InProceedings of the\\nACL-02 Workshop on Morphological and Phonolog-\\nical Learning, pages 21–30. Association for Compu-\\ntational Linguistics.\\nNadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp\\nKoehn. 2014. Integrating an Unsupervised Translit-\\neration Model into Statistical Machine Translation.\\nIn Proceedings of the 14th Conference of the Euro-\\npean Chapter of the Association for Computational\\nLinguistics, EACL 2014, pages 148–153, Gothen-\\nburg, Sweden.\\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\\n2013. A Simple, Fast, and Effective Reparame-\\nterization of IBM Model 2. InProceedings of the\\n2013 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 644–648, At-\\nlanta, Georgia. Association for Computational Lin-\\nguistics.\\nPhilip Gage. 1994. A New Algorithm for Data Com-\\npression.C Users J., 12(2):23–38, February.\\nBarry Haddow, Matthias Huck, Alexandra Birch, Niko-\\nlay Bogoychev, and Philipp Koehn. 2015. The\\nEdinburgh/JHU Phrase-based Machine Translation\\nSystems for WMT 2015. In Proceedings of the\\nTenth Workshop on Statistical Machine Translation,\\npages 126–133, Lisbon, Portugal. Association for\\nComputational Linguistics.\\nSébastien Jean, Kyunghyun Cho, Roland Memisevic,\\nand Yoshua Bengio. 2015. On Using Very Large\\nTarget V ocabulary for Neural Machine Translation.\\nIn Proceedings of the 53rd Annual Meeting of the\\nAssociation for Computational Linguistics and the\\n7th International Joint Conference on Natural Lan-\\nguage Processing (Volume 1: Long Papers), pages\\n1–10, Beijing, China. Association for Computa-\\ntional Linguistics.\\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\\nContinuous Translation Models. InProceedings of\\nthe 2013 Conference on Empirical Methods in Nat-\\nural Language Processing, Seattle. Association for\\nComputational Linguistics.\\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\\nder M. Rush. 2015. Character-Aware Neural Lan-\\nguage Models.CoRR , abs/1508.06615.\\nPhilipp Koehn and Kevin Knight. 2003. Empirical\\nMethods for Compound Splitting. InEACL ’03:\\nProceedings of the Tenth Conference on European\\nChapter of the Association for Computational Lin-\\nguistics, pages 187–193, Budapest, Hungary. Asso-\\nciation for Computational Linguistics.\\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\\nBrooke Cowan, Wade Shen, Christine Moran,\\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\\nConstantin, and Evan Herbst. 2007. Moses: Open\\nSource Toolkit for Statistical Machine Translation.\\nIn Proceedings of the ACL-2007 Demo and Poster\\nSessions, pages 177–180, Prague, Czech Republic.\\nAssociation for Computational Linguistics.\\nFranklin M. Liang. 1983.Word hy-phen-a-tion by\\ncom-put-er. Ph.D. thesis, Stanford University, De-\\npartment of Linguistics, Stanford, CA.\\nWang Ling, Chris Dyer, Alan W. Black, Isabel Tran-\\ncoso, Ramon Fermandez, Silvio Amir, Luis Marujo,\\nand Tiago Luis. 2015a. Finding Function in Form:\\nCompositional Character Models for Open V ocab-\\nulary Word Representation. InProceedings of the\\n2015 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP), pages 1520–\\n1530, Lisbon, Portugal. Association for Computa-\\ntional Linguistics.\\nWang Ling, Isabel Trancoso, Chris Dyer, and Alan W.\\nBlack. 2015b. Character-based Neural Machine\\nTranslation.ArXiv e-prints, November.\\nThang Luong, Richard Socher, and Christopher D.\\nManning. 2013. Better Word Representations\\nwith Recursive Neural Networks for Morphology.\\nIn Proceedings of the Seventeenth Conference on\\nComputational Natural Language Learning, CoNLL\\n2013, Soﬁa, Bulgaria, August 8-9, 2013, pages 104–\\n113.\\nThang Luong, Hieu Pham, and Christopher D. Man-\\nning. 2015a. Effective Approaches to Attention-\\nbased Neural Machine Translation. InProceed-\\nings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1412–\\n1421, Lisbon, Portugal. Association for Computa-\\ntional Linguistics.\\nThang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\\nand Wojciech Zaremba. 2015b. Addressing the\\nRare Word Problem in Neural Machine Translation.\\nIn Proceedings of the 53rd Annual Meeting of the\\nAssociation for Computational Linguistics and the\\n7th International Joint Conference on Natural Lan-\\nguage Processing (Volume 1: Long Papers), pages\\n11–19, Beijing, China. Association for Computa-\\ntional Linguistics.\\nTomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\\nSon Le, Stefan Kombrink, and Jan Cernocký. 2012.\\nSubword Language Modeling with Neural Net-\\nworks. Unpublished.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2016-06-13T01:31:02-04:00', 'moddate': '2016-06-13T01:31:02-04:00', 'title': 'arXiv:1508.07909v5  [cs.CL]  10 Jun 2016', 'source': 'G:\\\\Data Analyst CN\\\\Python\\\\LangChain\\\\Byte_Pair_Encoding.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Graham Neubig, Taro Watanabe, Shinsuke Mori, and\\nTatsuya Kawahara. 2012. Machine Translation\\nwithout Words through Substring Alignment. InThe\\n50th Annual Meeting of the Association for Compu-\\ntational Linguistics, Proceedings of the Conference,\\nJuly 8-14, 2012, Jeju Island, Korea - Volume 1: Long\\nPapers, pages 165–174.\\nSonja Nießen and Hermann Ney. 2000. Improving\\nSMT quality with morpho-syntactic analysis. In\\n18th Int. Conf. on Computational Linguistics, pages\\n1081–1085.\\nRazvan Pascanu, Tomas Mikolov, and Yoshua Ben-\\ngio. 2013. On the difﬁculty of training recurrent\\nneural networks. InProceedings of the 30th Inter-\\nnational Conference on Machine Learning, ICML\\n2013, pages 1310–1318, Atlanta, USA.\\nMaja Popovi´c. 2015. chrF: character n-gram F-score\\nfor automatic MT evaluation. InProceedings of the\\nTenth Workshop on Statistical Machine Translation,\\npages 392–395, Lisbon, Portugal. Association for\\nComputational Linguistics.\\nRico Sennrich and Barry Haddow. 2015. A Joint\\nDependency Model of Morphological and Syntac-\\ntic Structure for Statistical Machine Translation. In\\nProceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, pages\\n2081–2087, Lisbon, Portugal. Association for Com-\\nputational Linguistics.\\nBenjamin Snyder and Regina Barzilay. 2008. Unsu-\\npervised Multilingual Learning for Morphological\\nSegmentation. InProceedings of ACL-08: HLT,\\npages 737–745, Columbus, Ohio. Association for\\nComputational Linguistics.\\nDavid Stallard, Jacob Devlin, Michael Kayser,\\nYoong Keok Lee, and Regina Barzilay. 2012. Unsu-\\npervised Morphology Rivals Supervised Morphol-\\nogy for Arabic MT. InThe 50th Annual Meeting of\\nthe Association for Computational Linguistics, Pro-\\nceedings of the Conference, July 8-14, 2012, Jeju\\nIsland, Korea - Volume 2: Short Papers, pages 322–\\n327.\\nMiloš Stanojevi´c, Amir Kamran, Philipp Koehn, and\\nOnd ˇrej Bojar. 2015. Results of the WMT15 Met-\\nrics Shared Task. InProceedings of the Tenth Work-\\nshop on Statistical Machine Translation, pages 256–\\n273, Lisbon, Portugal. Association for Computa-\\ntional Linguistics.\\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\\nSequence to Sequence Learning with Neural Net-\\nworks. InAdvances in Neural Information Process-\\ning Systems 27: Annual Conference on Neural Infor-\\nmation Processing Systems 2014, pages 3104–3112,\\nMontreal, Quebec, Canada.\\nJörg Tiedemann. 2009. Character-based PSMT for\\nClosely Related Languages. InProceedings of 13th\\nAnnual Conference of the European Association for\\nMachine Translation (EAMT’09), pages 12–19.\\nJörg Tiedemann. 2012. Character-Based Pivot Trans-\\nlation for Under-Resourced Languages and Do-\\nmains. InProceedings of the 13th Conference of the\\nEuropean Chapter of the Association for Computa-\\ntional Linguistics, pages 141–151, Avignon, France.\\nAssociation for Computational Linguistics.\\nDavid Vilar, Jan-Thorsten Peter, and Hermann Ney.\\n2007. Can We Translate Letters? InSecond Work-\\nshop on Statistical Machine Translation, pages 33–\\n39, Prague, Czech Republic. Association for Com-\\nputational Linguistics.\\nSami Virpioja, Jaakko J. Väyrynen, Mathias Creutz,\\nand Markus Sadeniemi. 2007. Morphology-Aware\\nStatistical Machine Translation Based on Morphs\\nInduced in an Unsupervised Manner. InProceed-\\nings of the Machine Translation Summit XI, pages\\n491–498, Copenhagen, Denmark.\\nMatthew D. Zeiler. 2012. ADADELTA: An Adaptive\\nLearning Rate Method.CoRR , abs/1212.5701.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path =  \"G:\\Data Analyst CN\\Python\\LangChain\\Byte_Pair_Encoding.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658b48e",
   "metadata": {},
   "source": [
    "EMBEDDING THE DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c3561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_9560\\3684017633.py:7: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x12ec3591fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "textsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = textsplitter.split_documents(documents)\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042238d9",
   "metadata": {},
   "source": [
    "SAVE THE VECTORSTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc45d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"byte_pair_encoding_vectorstore\")\n",
    "# Load the vectorstore from the saved location\n",
    "# vectorstore = FAISS.load_local(\"byte_pair_encoding_vectorstore\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62963dca",
   "metadata": {},
   "source": [
    "EMBEDDING THE QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388a3ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3442440927028656,\n",
       " 1.855553388595581,\n",
       " -3.0146970748901367,\n",
       " -1.1159172058105469,\n",
       " 1.149372935295105,\n",
       " 0.27392542362213135,\n",
       " -1.534379005432129,\n",
       " -0.06926979124546051,\n",
       " -0.8770150542259216,\n",
       " -0.915999174118042,\n",
       " -0.7844842076301575,\n",
       " 1.9602628946304321,\n",
       " -0.24846473336219788,\n",
       " 0.18598106503486633,\n",
       " 0.15666267275810242,\n",
       " -0.8490591645240784,\n",
       " -1.0020636320114136,\n",
       " -0.5475313067436218,\n",
       " -0.027988990768790245,\n",
       " -1.213639736175537,\n",
       " -0.6544400453567505,\n",
       " 0.929191529750824,\n",
       " 0.24361839890480042,\n",
       " -0.819237232208252,\n",
       " 1.7831906080245972,\n",
       " 0.5073801875114441,\n",
       " -0.29540932178497314,\n",
       " 0.43027356266975403,\n",
       " -0.18504567444324493,\n",
       " 0.2895299792289734,\n",
       " 1.6135669946670532,\n",
       " -0.16319189965724945,\n",
       " 0.4346666634082794,\n",
       " -0.46110472083091736,\n",
       " -1.2377030849456787,\n",
       " -0.6822565793991089,\n",
       " 0.5361803770065308,\n",
       " -1.1412800550460815,\n",
       " -0.5721873044967651,\n",
       " 0.4100281298160553,\n",
       " -0.020976098254323006,\n",
       " 0.06717590987682343,\n",
       " -0.9983847141265869,\n",
       " -0.2013765424489975,\n",
       " 1.3185852766036987,\n",
       " 0.12965115904808044,\n",
       " 1.030908226966858,\n",
       " -0.9301697015762329,\n",
       " 1.491188406944275,\n",
       " -1.232968807220459,\n",
       " 0.7613105177879333,\n",
       " -0.1635284721851349,\n",
       " 1.026868462562561,\n",
       " 0.014450762420892715,\n",
       " 2.4152750968933105,\n",
       " 1.7626985311508179,\n",
       " -0.31386440992355347,\n",
       " 0.0077108596451580524,\n",
       " -0.5558499097824097,\n",
       " -0.027323750779032707,\n",
       " 0.7295482158660889,\n",
       " 1.3121378421783447,\n",
       " -0.3750724792480469,\n",
       " 1.6213538646697998,\n",
       " 0.6786441802978516,\n",
       " 0.670062243938446,\n",
       " 0.056164536625146866,\n",
       " 0.15938107669353485,\n",
       " 1.3983417749404907,\n",
       " -0.8184639811515808,\n",
       " 1.303463101387024,\n",
       " -0.3724959194660187,\n",
       " 0.33332300186157227,\n",
       " 1.2715134620666504,\n",
       " -0.7437653541564941,\n",
       " -0.4856736958026886,\n",
       " -1.2241562604904175,\n",
       " 0.8267821669578552,\n",
       " 0.09481926262378693,\n",
       " 1.737023115158081,\n",
       " 0.11324853450059891,\n",
       " 0.22518841922283173,\n",
       " 0.5544863343238831,\n",
       " -0.18421036005020142,\n",
       " 0.29740944504737854,\n",
       " -0.22124890983104706,\n",
       " -0.15504595637321472,\n",
       " -0.06484609097242355,\n",
       " -1.2127219438552856,\n",
       " 0.9344063997268677,\n",
       " 0.0002265961520606652,\n",
       " -1.678488850593567,\n",
       " 0.6813163161277771,\n",
       " 0.6443731188774109,\n",
       " -0.5830931663513184,\n",
       " 0.790308952331543,\n",
       " -0.2583085894584656,\n",
       " 1.428691029548645,\n",
       " 0.5606417655944824,\n",
       " -0.16972820460796356,\n",
       " -1.1649736166000366,\n",
       " 0.919476330280304,\n",
       " 0.4408411979675293,\n",
       " -0.37407827377319336,\n",
       " -0.2347254902124405,\n",
       " 0.3437305986881256,\n",
       " 0.2518357038497925,\n",
       " 0.688389003276825,\n",
       " -1.2158676385879517,\n",
       " -0.3826831579208374,\n",
       " -2.0129270553588867,\n",
       " 0.26113593578338623,\n",
       " -0.9592522382736206,\n",
       " -0.885830283164978,\n",
       " 0.028758786618709564,\n",
       " 0.35656067728996277,\n",
       " 0.6546834707260132,\n",
       " -0.6194874048233032,\n",
       " -1.0813801288604736,\n",
       " 0.7007189989089966,\n",
       " -0.5595108866691589,\n",
       " -1.4752840995788574,\n",
       " 0.07924994826316833,\n",
       " 1.3342610597610474,\n",
       " 0.6361086368560791,\n",
       " 0.7971473932266235,\n",
       " -1.1783632040023804,\n",
       " 0.483770489692688,\n",
       " 0.13464079797267914,\n",
       " -0.8299741148948669,\n",
       " 0.294723778963089,\n",
       " -1.0578322410583496,\n",
       " 0.30980685353279114,\n",
       " -0.5970420241355896,\n",
       " 0.24852006137371063,\n",
       " 0.45385149121284485,\n",
       " -0.2561439275741577,\n",
       " 0.41134339570999146,\n",
       " 1.377866268157959,\n",
       " 0.8291446566581726,\n",
       " 0.052528392523527145,\n",
       " -0.11168858408927917,\n",
       " -0.6060832142829895,\n",
       " -1.089137315750122,\n",
       " 1.734389305114746,\n",
       " -0.4397440254688263,\n",
       " -0.9610103368759155,\n",
       " -0.1948767751455307,\n",
       " -1.0365726947784424,\n",
       " 0.7269734144210815,\n",
       " -0.05149926245212555,\n",
       " 1.453567624092102,\n",
       " 0.079095259308815,\n",
       " -0.36312347650527954,\n",
       " 0.24386249482631683,\n",
       " 0.15246762335300446,\n",
       " 0.5706106424331665,\n",
       " -0.3979930281639099,\n",
       " 0.6175808310508728,\n",
       " 1.0446456670761108,\n",
       " 0.44967108964920044,\n",
       " -0.5956989526748657,\n",
       " -0.11358260363340378,\n",
       " 0.6557170152664185,\n",
       " -0.5020238161087036,\n",
       " -0.9738864302635193,\n",
       " 1.0851612091064453,\n",
       " 0.15693864226341248,\n",
       " 0.01291425246745348,\n",
       " 0.8347461819648743,\n",
       " -0.891364336013794,\n",
       " -0.5004969835281372,\n",
       " -0.37622910737991333,\n",
       " -0.21218985319137573,\n",
       " -0.16592760384082794,\n",
       " 1.5282964706420898,\n",
       " 0.4493197798728943,\n",
       " -0.6537113785743713,\n",
       " -0.1605439931154251,\n",
       " -0.9172494411468506,\n",
       " -0.39981961250305176,\n",
       " -0.39071768522262573,\n",
       " 0.931515634059906,\n",
       " 1.5366735458374023,\n",
       " -0.3603488802909851,\n",
       " -0.3745866119861603,\n",
       " -0.3289015293121338,\n",
       " 0.0013345653424039483,\n",
       " -1.2884759902954102,\n",
       " -0.9847270250320435,\n",
       " -0.33540457487106323,\n",
       " -0.45904311537742615,\n",
       " -1.2312321662902832,\n",
       " -1.364007592201233,\n",
       " -1.2861071825027466,\n",
       " -1.0412521362304688,\n",
       " 1.0190365314483643,\n",
       " 0.9280617833137512,\n",
       " 1.7580221891403198,\n",
       " -0.993705153465271,\n",
       " -0.8275952935218811,\n",
       " 0.14085279405117035,\n",
       " -0.897710919380188,\n",
       " 1.057331919670105,\n",
       " 0.4820863902568817,\n",
       " 0.6497431993484497,\n",
       " -0.882699728012085,\n",
       " -0.010541018098592758,\n",
       " -0.751314640045166,\n",
       " -0.54323410987854,\n",
       " 0.04068928584456444,\n",
       " -0.6835681796073914,\n",
       " -1.1354122161865234,\n",
       " 0.1576346755027771,\n",
       " -0.3227078318595886,\n",
       " -1.787833333015442,\n",
       " -0.820103108882904,\n",
       " -1.0861011743545532,\n",
       " -0.9711524248123169,\n",
       " 0.6520479917526245,\n",
       " 0.7896712422370911,\n",
       " -0.7682396173477173,\n",
       " 0.6774142384529114,\n",
       " 0.7068953514099121,\n",
       " 0.4611189365386963,\n",
       " -0.0064766984432935715,\n",
       " 0.4494100511074066,\n",
       " -0.08751408755779266,\n",
       " -0.31321680545806885,\n",
       " 0.5608006715774536,\n",
       " -0.19479507207870483,\n",
       " -0.9293176531791687,\n",
       " 0.542954683303833,\n",
       " 0.46633508801460266,\n",
       " -1.010321021080017,\n",
       " 0.9178184866905212,\n",
       " 0.8987500071525574,\n",
       " 2.0032453536987305,\n",
       " -0.3624151349067688,\n",
       " 0.009155983105301857,\n",
       " 0.45903027057647705,\n",
       " 1.5215511322021484,\n",
       " -1.2360074520111084,\n",
       " 0.4175136983394623,\n",
       " 0.10589931905269623,\n",
       " 1.4188400506973267,\n",
       " 0.370459645986557,\n",
       " -1.7273941040039062,\n",
       " -0.5199078917503357,\n",
       " -0.021033896133303642,\n",
       " -0.5154709815979004,\n",
       " -1.3209580183029175,\n",
       " 0.1639476716518402,\n",
       " -0.08431024849414825,\n",
       " 0.7275469303131104,\n",
       " -0.4428479075431824,\n",
       " -0.11155831813812256,\n",
       " -0.2801789939403534,\n",
       " -1.1756415367126465,\n",
       " -0.7758760452270508,\n",
       " 1.1289441585540771,\n",
       " 0.11106742173433304,\n",
       " 0.5762795209884644,\n",
       " -0.8725088834762573,\n",
       " -0.015550578944385052,\n",
       " -0.2447420209646225,\n",
       " 1.8201743364334106,\n",
       " -0.3861551284790039,\n",
       " -0.2044229507446289,\n",
       " 0.4013698697090149,\n",
       " 0.5373614430427551,\n",
       " 0.9854166507720947,\n",
       " -0.10482650995254517,\n",
       " -0.5941320657730103,\n",
       " 0.056049544364213943,\n",
       " -0.21361394226551056,\n",
       " 0.34822672605514526,\n",
       " -0.5862209796905518,\n",
       " -1.3521836996078491,\n",
       " 0.5591935515403748,\n",
       " -1.1694848537445068,\n",
       " 1.02550208568573,\n",
       " -1.5174434185028076,\n",
       " -0.45060551166534424,\n",
       " 0.5543013215065002,\n",
       " 1.3100006580352783,\n",
       " 0.28455477952957153,\n",
       " 0.8975988626480103,\n",
       " 1.7115832567214966,\n",
       " 0.4972224533557892,\n",
       " 0.41971251368522644,\n",
       " -1.3616997003555298,\n",
       " 0.3224382996559143,\n",
       " 1.700427532196045,\n",
       " 0.17105664312839508,\n",
       " 0.6075527667999268,\n",
       " 1.090266227722168,\n",
       " 0.054733891040086746,\n",
       " 0.10949364304542542,\n",
       " -0.5269348621368408,\n",
       " 0.12193913757801056,\n",
       " 0.14047862589359283,\n",
       " 1.3458808660507202,\n",
       " 0.20917236804962158,\n",
       " 0.14604437351226807,\n",
       " -0.0853872299194336,\n",
       " 0.3394027352333069,\n",
       " -0.20248757302761078,\n",
       " -0.8895443081855774,\n",
       " -0.8603825569152832,\n",
       " -1.3765273094177246,\n",
       " -0.38483861088752747,\n",
       " -0.4413381516933441,\n",
       " 0.5469119548797607,\n",
       " -0.5670087337493896,\n",
       " 0.10697424411773682,\n",
       " 1.55072820186615,\n",
       " -0.15848013758659363,\n",
       " 1.1710362434387207,\n",
       " -0.4647514820098877,\n",
       " 0.2198837399482727,\n",
       " -0.40912261605262756,\n",
       " 0.0927581787109375,\n",
       " -1.7352570295333862,\n",
       " 0.33962729573249817,\n",
       " 1.0580726861953735,\n",
       " -0.7803429961204529,\n",
       " -0.11527855694293976,\n",
       " -0.9350305199623108,\n",
       " 0.425752729177475,\n",
       " 0.013304886408150196,\n",
       " 0.5498653054237366,\n",
       " 0.5967354774475098,\n",
       " 0.3659840226173401,\n",
       " -1.0681023597717285,\n",
       " -0.02868589013814926,\n",
       " -1.023333191871643,\n",
       " 0.37630605697631836,\n",
       " 0.017475608736276627,\n",
       " 1.5933929681777954,\n",
       " 0.812210738658905,\n",
       " 0.5735405087471008,\n",
       " -0.22908233106136322,\n",
       " -0.8499746918678284,\n",
       " 0.39020198583602905,\n",
       " 0.7669854760169983,\n",
       " -0.47859466075897217,\n",
       " -0.972366213798523,\n",
       " 0.3746229112148285,\n",
       " -0.1265970766544342,\n",
       " -1.1420862674713135,\n",
       " 1.2606061697006226,\n",
       " 0.8096732497215271,\n",
       " -0.6363567113876343,\n",
       " -0.8236868381500244,\n",
       " 0.6460937261581421,\n",
       " 1.0615085363388062,\n",
       " -1.2866796255111694,\n",
       " 0.7504648566246033,\n",
       " -0.2270497977733612,\n",
       " 0.6269784569740295,\n",
       " 0.16327932476997375,\n",
       " -1.189151644706726,\n",
       " -0.15273861587047577,\n",
       " -1.0469205379486084,\n",
       " -1.10832679271698,\n",
       " 0.4068709909915924,\n",
       " -1.0396537780761719,\n",
       " 0.650845468044281,\n",
       " -0.10753018409013748,\n",
       " -1.755593180656433,\n",
       " -0.4828636348247528,\n",
       " 0.0715022161602974,\n",
       " 1.1062997579574585,\n",
       " 0.16729791462421417,\n",
       " -0.6298184394836426,\n",
       " -1.490267276763916,\n",
       " -0.6709355711936951,\n",
       " 0.194559246301651,\n",
       " 1.2382125854492188,\n",
       " -0.5299345850944519,\n",
       " -0.6664120554924011,\n",
       " 1.794750452041626,\n",
       " 0.10896997153759003,\n",
       " 0.3927585780620575,\n",
       " -0.9505573511123657,\n",
       " 0.720928430557251,\n",
       " -0.21938231587409973,\n",
       " 0.9300428628921509,\n",
       " 0.6615774035453796,\n",
       " 0.0092331413179636,\n",
       " -0.4886072874069214,\n",
       " -1.351854681968689,\n",
       " -0.41798070073127747,\n",
       " 0.36125925183296204,\n",
       " 0.26181724667549133,\n",
       " 0.29073527455329895,\n",
       " 0.5318431854248047,\n",
       " 0.0679333284497261,\n",
       " 0.6926037669181824,\n",
       " -0.6270579099655151,\n",
       " -1.3862478733062744,\n",
       " -1.5294926166534424,\n",
       " -0.9307941198348999,\n",
       " -0.8533975481987,\n",
       " 0.9752489924430847,\n",
       " -1.3036028146743774,\n",
       " -1.468202829360962,\n",
       " 1.3466002941131592,\n",
       " 0.2133999466896057,\n",
       " 0.2702113687992096,\n",
       " 1.07903254032135,\n",
       " 0.13815255463123322,\n",
       " -0.9686567187309265,\n",
       " -0.3300243318080902,\n",
       " 0.2903704345226288,\n",
       " 0.17119090259075165,\n",
       " 0.7075814604759216,\n",
       " -0.6151223182678223,\n",
       " 0.6018322706222534,\n",
       " -0.4255788028240204,\n",
       " 1.7407821416854858,\n",
       " -1.0420485734939575,\n",
       " -0.11228365451097488,\n",
       " 0.02480231784284115,\n",
       " 1.8281691074371338,\n",
       " 0.28417691588401794,\n",
       " 1.1001396179199219,\n",
       " 0.6732532978057861,\n",
       " -1.008062720298767,\n",
       " 0.5543479919433594,\n",
       " 0.3742060661315918,\n",
       " 0.4067555069923401,\n",
       " 0.692565381526947,\n",
       " -0.7969803214073181,\n",
       " -0.94340580701828,\n",
       " -0.10426007956266403,\n",
       " 0.24896012246608734,\n",
       " -0.8620030879974365,\n",
       " 1.500292420387268,\n",
       " 0.5373431444168091,\n",
       " -1.2657079696655273,\n",
       " -0.6526530981063843,\n",
       " 0.318093866109848,\n",
       " 1.0236984491348267,\n",
       " 1.7852084636688232,\n",
       " 1.2374752759933472,\n",
       " -1.2425848245620728,\n",
       " -0.5779511332511902,\n",
       " 1.3109660148620605,\n",
       " 1.2293994426727295,\n",
       " -0.15402266383171082,\n",
       " 1.1678383350372314,\n",
       " 0.2736549377441406,\n",
       " 2.2718846797943115,\n",
       " 0.7616419196128845,\n",
       " 1.1381597518920898,\n",
       " -0.02189650759100914,\n",
       " 0.34330955147743225,\n",
       " 0.7795552611351013,\n",
       " -0.1946251094341278,\n",
       " 1.0529934167861938,\n",
       " -1.1050610542297363,\n",
       " 1.1645718812942505,\n",
       " 0.09885093569755554,\n",
       " -0.19656381011009216,\n",
       " -0.04264761134982109,\n",
       " -1.7601896524429321,\n",
       " 0.509151041507721,\n",
       " 1.2543935775756836,\n",
       " -0.7869518399238586,\n",
       " -0.7203811407089233,\n",
       " 0.8908671736717224,\n",
       " -0.47561290860176086,\n",
       " 0.8267327547073364,\n",
       " 0.8473454713821411,\n",
       " -0.790591835975647,\n",
       " -0.6159741878509521,\n",
       " 0.648711621761322,\n",
       " 0.6921124458312988,\n",
       " 0.04158894345164299,\n",
       " 0.5526486039161682,\n",
       " -0.445325642824173,\n",
       " -0.566960334777832,\n",
       " -0.642545759677887,\n",
       " 1.8365485668182373,\n",
       " 0.04902999475598335,\n",
       " 0.4002262055873871,\n",
       " 0.5767565369606018,\n",
       " 0.024931350722908974,\n",
       " 0.9892895221710205,\n",
       " -0.06759396195411682,\n",
       " 0.6995803713798523,\n",
       " -0.3927149474620819,\n",
       " -0.8420515656471252,\n",
       " -0.6264340877532959,\n",
       " -0.3705238401889801,\n",
       " 0.3538419008255005,\n",
       " -0.8508347272872925,\n",
       " 0.24096553027629852,\n",
       " 0.8181034326553345,\n",
       " -0.0801430270075798,\n",
       " 0.8543986082077026,\n",
       " 1.3182449340820312,\n",
       " 1.4692813158035278,\n",
       " 0.8423848748207092,\n",
       " 1.2773054838180542,\n",
       " -0.6354364156723022,\n",
       " -0.7874191999435425,\n",
       " -0.2634873688220978,\n",
       " -1.0101696252822876,\n",
       " -0.6358723044395447,\n",
       " 0.4289364516735077,\n",
       " 0.6645154356956482,\n",
       " 0.0847969725728035,\n",
       " -0.4123712182044983,\n",
       " 0.05946364253759384,\n",
       " -0.13126066327095032,\n",
       " -0.6393911838531494,\n",
       " -0.7254974842071533,\n",
       " -0.2919909358024597,\n",
       " -0.026461727917194366,\n",
       " 0.4008052945137024,\n",
       " 0.85789555311203,\n",
       " -0.2329103946685791,\n",
       " -0.3592424690723419,\n",
       " 0.7733083367347717,\n",
       " 1.1559762954711914,\n",
       " 0.5123215913772583,\n",
       " 0.6670344471931458,\n",
       " 0.14763353765010834,\n",
       " -0.6164474487304688,\n",
       " -0.33551111817359924,\n",
       " -0.746432900428772,\n",
       " 0.03389745578169823,\n",
       " -0.37642785906791687,\n",
       " -0.1563529372215271,\n",
       " 0.8733888864517212,\n",
       " 0.3216053545475006,\n",
       " 1.0643585920333862,\n",
       " 0.0042473915964365005,\n",
       " 0.4956371784210205,\n",
       " -0.6084020733833313,\n",
       " 0.20542338490486145,\n",
       " -0.8334876894950867,\n",
       " -0.25872039794921875,\n",
       " -1.0951457023620605,\n",
       " -0.7166861891746521,\n",
       " 0.21960534155368805,\n",
       " -0.9382892847061157,\n",
       " 0.45227816700935364,\n",
       " -0.6794947981834412,\n",
       " -0.3176804184913635,\n",
       " -0.5756181478500366,\n",
       " 1.4211612939834595,\n",
       " -0.6909701228141785,\n",
       " -0.39552026987075806,\n",
       " -0.5057559609413147,\n",
       " 0.5750447511672974,\n",
       " 0.4582862854003906,\n",
       " 0.6631141901016235,\n",
       " 0.5927835702896118,\n",
       " 0.9895846247673035,\n",
       " 1.0397785902023315,\n",
       " 1.7236930131912231,\n",
       " -0.8573609590530396,\n",
       " -0.47432732582092285,\n",
       " -0.1801346093416214,\n",
       " 0.1265120655298233,\n",
       " 1.3766257762908936,\n",
       " -0.9045289754867554,\n",
       " -1.2160652875900269,\n",
       " -0.737512469291687,\n",
       " 0.18207547068595886,\n",
       " -0.02125757373869419,\n",
       " -0.26343467831611633,\n",
       " -0.20390869677066803,\n",
       " 0.1450413167476654,\n",
       " -1.7659565210342407,\n",
       " -0.3561677634716034,\n",
       " -0.3783196210861206,\n",
       " -0.40114378929138184,\n",
       " -1.4300875663757324,\n",
       " 0.8599993586540222,\n",
       " -0.8685787916183472,\n",
       " -0.09558426588773727,\n",
       " -0.7996999025344849,\n",
       " -1.0969128608703613,\n",
       " 1.1271154880523682,\n",
       " -0.22746987640857697,\n",
       " -0.5222878456115723,\n",
       " 0.4603428244590759,\n",
       " 0.5328561067581177,\n",
       " -0.3614341914653778,\n",
       " -0.08310339599847794,\n",
       " 0.5848724246025085,\n",
       " -0.16924045979976654,\n",
       " -0.9143171310424805,\n",
       " -0.48370423913002014,\n",
       " -1.4829580783843994,\n",
       " -0.5554563999176025,\n",
       " 1.104231834411621,\n",
       " 1.093507170677185,\n",
       " -0.6342087388038635,\n",
       " 0.24079710245132446,\n",
       " 1.4663972854614258,\n",
       " -0.45451393723487854,\n",
       " -0.6856191754341125,\n",
       " -0.8559504151344299,\n",
       " 0.33328384160995483,\n",
       " 0.30611154437065125,\n",
       " -0.5498149394989014,\n",
       " -1.1432064771652222,\n",
       " 0.28700533509254456,\n",
       " 0.7937623262405396,\n",
       " -0.564676821231842,\n",
       " 2.073215961456299,\n",
       " -0.12976506352424622,\n",
       " -0.5732371211051941,\n",
       " -0.010201264172792435,\n",
       " -0.4621739983558655,\n",
       " -1.3270947933197021,\n",
       " 0.07257615774869919,\n",
       " -1.5530664920806885,\n",
       " 0.6469997763633728,\n",
       " 0.694955050945282,\n",
       " -0.7257603406906128,\n",
       " -0.14825394749641418,\n",
       " 1.1291483640670776,\n",
       " 0.9492585062980652,\n",
       " -0.6264773607254028,\n",
       " 0.7698289752006531,\n",
       " -1.0478260517120361,\n",
       " -0.7491304874420166,\n",
       " -1.3847343921661377,\n",
       " 1.3391417264938354,\n",
       " -1.5292158126831055,\n",
       " 0.3603730797767639,\n",
       " -0.3678124248981476,\n",
       " 1.6605396270751953,\n",
       " 0.6203955411911011,\n",
       " 0.5928897261619568,\n",
       " -0.20692162215709686,\n",
       " 0.3239775598049164,\n",
       " 0.07840287685394287,\n",
       " -0.2571468651294708,\n",
       " 0.3700389266014099,\n",
       " 0.033349279314279556,\n",
       " 0.781578004360199,\n",
       " -0.6633507013320923,\n",
       " 0.5610576868057251,\n",
       " 0.7971640825271606,\n",
       " -0.419397234916687,\n",
       " 0.2144852578639984,\n",
       " -0.04419953003525734,\n",
       " -1.016768217086792,\n",
       " 0.07281273603439331,\n",
       " -0.3554130792617798,\n",
       " -1.1248067617416382,\n",
       " -1.067409873008728,\n",
       " -0.2449747771024704,\n",
       " -0.47467100620269775,\n",
       " -0.640329122543335,\n",
       " -0.8761903047561646,\n",
       " 1.3242862224578857,\n",
       " -0.37250781059265137,\n",
       " -0.21281079947948456,\n",
       " -1.398284912109375,\n",
       " -1.1896077394485474,\n",
       " -0.6718199253082275,\n",
       " 0.7502151727676392,\n",
       " -0.591464638710022,\n",
       " 0.6578085422515869,\n",
       " -1.0993136167526245,\n",
       " 0.6647109985351562,\n",
       " 0.17697672545909882,\n",
       " 0.468555212020874,\n",
       " 0.7183964252471924,\n",
       " -0.825657069683075,\n",
       " -1.2454845905303955,\n",
       " 0.16466940939426422,\n",
       " 0.3805778920650482,\n",
       " 0.3666798770427704,\n",
       " 0.24080771207809448,\n",
       " -0.4661117196083069,\n",
       " -0.18443730473518372,\n",
       " 0.9037142992019653,\n",
       " -0.48026785254478455,\n",
       " 0.4917893409729004,\n",
       " -0.7361881136894226,\n",
       " -0.28098243474960327,\n",
       " -0.9637981653213501,\n",
       " -0.05675308033823967,\n",
       " -0.31992581486701965,\n",
       " 0.31218090653419495,\n",
       " 0.6583415865898132,\n",
       " 0.047458723187446594,\n",
       " -0.04581357166171074,\n",
       " -0.6954733729362488,\n",
       " 1.3547999858856201,\n",
       " -0.659186601638794,\n",
       " -0.7384933829307556,\n",
       " 0.12294565141201019,\n",
       " 1.3583179712295532,\n",
       " -0.0288972407579422,\n",
       " -0.5691776871681213,\n",
       " 1.172608733177185,\n",
       " 0.28198352456092834,\n",
       " -0.7535619735717773,\n",
       " -1.6936330795288086,\n",
       " -0.6303640007972717,\n",
       " -1.0113904476165771,\n",
       " 0.2727885842323303,\n",
       " 1.0113459825515747,\n",
       " -0.6267569661140442,\n",
       " -0.7706315517425537,\n",
       " 0.7726903557777405,\n",
       " 0.05195552855730057,\n",
       " -1.1403475999832153,\n",
       " 0.6628183722496033,\n",
       " 1.9434471130371094,\n",
       " 0.055705487728118896,\n",
       " -0.21166782081127167,\n",
       " 0.44277048110961914,\n",
       " -0.42162007093429565,\n",
       " -0.9580327272415161,\n",
       " -0.09264922142028809,\n",
       " 0.07931602001190186,\n",
       " 0.43685588240623474,\n",
       " -0.2677086293697357,\n",
       " -0.5501065254211426,\n",
       " -0.3021121621131897,\n",
       " 0.34209683537483215,\n",
       " 0.8423699140548706,\n",
       " -0.3310856819152832,\n",
       " 1.2838983535766602,\n",
       " -0.9833672642707825,\n",
       " 1.1426141262054443,\n",
       " -1.096032977104187,\n",
       " 0.9192506074905396,\n",
       " -0.6285030841827393,\n",
       " 1.0911225080490112,\n",
       " -0.05183630436658859,\n",
       " 0.22555971145629883,\n",
       " -0.17027869820594788,\n",
       " -1.212338924407959,\n",
       " -0.5448975563049316,\n",
       " 0.10988561064004898,\n",
       " 0.9928646683692932,\n",
       " -0.48089659214019775,\n",
       " 0.829102098941803,\n",
       " -0.7140183448791504,\n",
       " -0.7393057942390442,\n",
       " 1.0524927377700806,\n",
       " -0.65850430727005,\n",
       " 0.8496097922325134,\n",
       " 0.6942417025566101,\n",
       " 0.7774619460105896,\n",
       " 2.39556884765625,\n",
       " -0.5371162295341492,\n",
       " -0.7347755432128906,\n",
       " -0.7277342677116394,\n",
       " 0.2711281478404999,\n",
       " 0.2443077117204666,\n",
       " 0.2669782042503357,\n",
       " 0.07393577694892883,\n",
       " -0.6206457614898682,\n",
       " -0.256149023771286]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question = \"What is Byte Pair Encoding?\"\n",
    "embedded_question = embeddings.embed_query(my_question)\n",
    "embedded_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76b751",
   "metadata": {},
   "source": [
    "MATCHING THE QUESTION WITH THE VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91879a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increase time and space efﬁciency, and to allow for\\ntranslation without back-off models. At the same\\ntime, we also want a compact representation of the\\ntext itself, since an increase in text length reduces\\nefﬁciency and increases the distances over which\\nneural models need to pass information.\\nA simple method to manipulate the trade-off be-\\ntween vocabulary size and text size is to use short-\\nlists of unsegmented words, using subword units\\nonly for rare words. As an alternative, we pro-\\npose a segmentation algorithm based on byte pair\\nencoding (BPE), which lets us learn a vocabulary\\nthat provides a good compression rate of the text.\\n3.2 Byte Pair Encoding (BPE)\\nByte Pair Encoding (BPE) (Gage, 1994) is a sim-\\nple data compression technique that iteratively re-\\nplaces the most frequent pair of bytes in a se-\\nquence with a single, unused byte. We adapt this\\nalgorithm for word segmentation. Instead of merg-\\ning frequent pairs of bytes, we merge characters or\\ncharacter sequences.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Retrieve_docs = vectorstore.similarity_search_by_vector(embedded_question, k=3)\n",
    "Retrieve_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a1488",
   "metadata": {},
   "source": [
    "Merging all the retrieved_docs in a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b92dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context = \"\"\n",
    "for doc in Retrieve_docs:\n",
    "    all_context += doc.page_content + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1372f8e",
   "metadata": {},
   "source": [
    "CREATING THE RAG PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cce6207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8314b",
   "metadata": {},
   "source": [
    "Constructing the message with Context and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b75dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is Byte Pair Encoding? \\nContext: increase time and space efﬁciency, and to allow for\\ntranslation without back-off models. At the same\\ntime, we also want a compact representation of the\\ntext itself, since an increase in text length reduces\\nefﬁciency and increases the distances over which\\nneural models need to pass information.\\nA simple method to manipulate the trade-off be-\\ntween vocabulary size and text size is to use short-\\nlists of unsegmented words, using subword units\\nonly for rare words. As an alternative, we pro-\\npose a segmentation algorithm based on byte pair\\nencoding (BPE), which lets us learn a vocabulary\\nthat provides a good compression rate of the text.\\n3.2 Byte Pair Encoding (BPE)\\nByte Pair Encoding (BPE) (Gage, 1994) is a sim-\\nple data compression technique that iteratively re-\\nplaces the most frequent pair of bytes in a se-\\nquence with a single, unused byte. We adapt this\\nalgorithm for word segmentation. Instead of merg-\\ning frequent pairs of bytes, we merge characters or\\ncharacter sequences.\\ncrementally.\\nThe main difference to other compression al-\\ngorithms, such as Huffman encoding, which have\\nbeen proposed to produce a variable-length en-\\ncoding of words for NMT (Chitnis and DeNero,\\n2015), is that our symbol sequences are still in-\\nterpretable as subword units, and that the network\\ncan generalize to translate and produce new words\\n(unseen at training time) on the basis of these sub-\\nword units.\\nFigure 1 shows a toy example of learned BPE\\noperations. At test time, we ﬁrst split words into\\nsequences of characters, then apply the learned op-\\nerations to merge the characters into larger, known\\nsymbols. This is applicable to any word, and\\nallows for open-vocabulary networks with ﬁxed\\nsymbol vocabularies.3 In our example, the OOV\\n‘lower’ would be segmented into ‘low er·’.\\n3The only symbols that will be unknown at test time are\\nunknown characters, or symbols of which all occurrences\\nin the training text have been merged into larger symbols,\\ning frequent pairs of bytes, we merge characters or\\ncharacter sequences.\\nFirstly, we initialize the symbol vocabulary with\\nthe character vocabulary, and represent each word\\nas a sequence of characters, plus a special end-of-\\nword symbol ‘·’, which allows us to restore the\\noriginal tokenization after translation. We itera-\\ntively count all symbol pairs and replace each oc-\\ncurrence of the most frequent pair (‘A’, ‘B’) with\\na new symbol ‘AB’. Each merge operation pro-\\nduces a new symbol which represents a charac-\\ntern-gram. Frequent charactern-grams (or whole\\nwords) are eventually merged into a single sym-\\nbol, thus BPE requires no shortlist. The ﬁnal sym-\\nbol vocabulary size is equal to the size of the initial\\nvocabulary, plus the number of merge operations\\n– the latter is the only hyperparameter of the algo-\\nrithm.\\nFor efﬁciency, we do not consider pairs that\\ncross word boundaries. The algorithm can thus be\\nrun on the dictionary extracted from a text, with\\n \\nAnswer:\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = prompt.invoke(\n",
    "    {\n",
    "        \"question\": my_question,\n",
    "        \"context\": all_context\n",
    "    }\n",
    ")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14af54",
   "metadata": {},
   "source": [
    "Invoking the LLM(Ollama) with the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650c89bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_9560\\2646123809.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:1b\", temperature=0.1)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", temperature=0.1)\n",
    "result = llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95c85aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Byte Pair Encoding (BPE) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. This process is repeated for each character or character sequence in the input text, allowing for efficient and compact representation of the text while maintaining a good trade-off between vocabulary size and text size. BPE can be used to learn a vocabulary that provides a good compression rate of the text by merging frequent pairs of bytes into single symbols.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
